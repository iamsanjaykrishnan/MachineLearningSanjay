{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Neural network model was trained with 100 quotes to predict author based on the quote\n",
    "\n",
    "lim : It was not tested with validation data and the training is made only for checking lstm \n",
    "    therefore datasets and other options such as attention has not been implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read text from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "logging.basicConfig(level=logging.CRITICAL,)\n",
    "\n",
    "path='InputText.txt'                   \n",
    "input_text = open(path, 'r') \n",
    "raw_text=input_text.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove empty lines from raw text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Striped_text=[]\n",
    "\n",
    "for line in raw_text:\n",
    "    if not(line.strip()):\n",
    "        pass\n",
    "    else:\n",
    "        Striped_text.append(line)\n",
    "        \n",
    "raw_text=Striped_text\n",
    "logging.debug(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import text processing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text \n",
    "- RawTextFormat : Nr. 'quote' -- Author\n",
    "- Remove numbers\n",
    "- make a dictionary for each sentence \n",
    "       - quote \n",
    "       - author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_senteces=[]                                 # empty list for storing processed dict for each quote\n",
    "logging.debug(len(raw_text))\n",
    "\n",
    "for i in range(len(raw_text)):\n",
    "    sentence = raw_text[i]\n",
    "    tokenized_sentence = sent_tokenize(sentence)\n",
    "    logging.debug(tokenized_sentence)\n",
    "    \n",
    "    for j in range(len(tokenized_sentence)):         # remove \"\" and -- from each sentence\n",
    "        tokenized_sentence[j] = tokenized_sentence[j].replace('\"','')\n",
    "        tokenized_sentence[j] = tokenized_sentence[j].replace('-- ','')\n",
    "        tokenized_sentence[j] = tokenized_sentence[j].replace('--','')\n",
    "        \n",
    "    quote=[]                                        # create list to store characters of quote\n",
    "    for k in range(1,len(tokenized_sentence)-1):\n",
    "        quote += tokenized_sentence[k]\n",
    "    quote = _.join(quote)                           # join the characters in quote to str\n",
    "    logging.debug(quote)\n",
    "    \n",
    "    content = {'quote':quote, 'author':tokenized_sentence[len(tokenized_sentence)-1]}\n",
    "                                                    # Dictionary to store quote and author name for each \n",
    "                                                    # sentence\n",
    "    processed_senteces.append(content)\n",
    "    \n",
    "logging.debug(processed_senteces)\n",
    "logging.debug(processed_senteces[1]['quote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create own gensim wordvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipogramText = []                                  # create list for word vectors to be derived from\n",
    "\n",
    "for i in range(len(processed_senteces)):            # Word tokenize the sentences\n",
    "    processed_senteces[i]['quote'] = word_tokenize(processed_senteces[i]['quote'])\n",
    "    temp = list(processed_senteces[i]['quote'])\n",
    "    temp.append(processed_senteces[i]['author'])\n",
    "    skipogramText.append(temp)\n",
    "    \n",
    "logging.debug(skipogramText[1])\n",
    "logging.debug(processed_senteces[0]['quote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjay\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(skipogramText, min_count=1, sg=0)\n",
    "logging.debug(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for reading vector and using similarity in gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "#print(processed_senteces[0]['quote'])\n",
    "vector = model.wv.get_vector('Success')                              # Geting the word vector\n",
    "print(vector.shape)\n",
    "similarWord = model.wv.most_similar(positive=[vector], topn=100)     # Vector to word\n",
    "#print(similarWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorizedQuote = []                                                # create Word vector for all words in \n",
    "                                                                    # the processed sentences\n",
    "VectorizedAuthor = []  \n",
    "for i in range(len(processed_senteces)):\n",
    "    len_quote=len(processed_senteces[i]['quote'])\n",
    "    temp = np.empty([len_quote,100])\n",
    "    for j in range(len_quote):\n",
    "        temp[j][:]=model.wv.get_vector(processed_senteces[i]['quote'][j])\n",
    "    VectorizedQuote.append(np.copy(temp))\n",
    "    temp=model.wv.get_vector(processed_senteces[i]['author'])\n",
    "    VectorizedAuthor.append(np.copy(temp))\n",
    "    logging.debug(model.wv.most_similar(positive=[VectorizedAuthor[i]], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a neural network that can read quotes and predict author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf                                             # Create a tf neural network model \n",
    "\n",
    "tf.reset_default_graph()\n",
    "inputVec = tf.placeholder(tf.float32, shape=(None, None, 100))      # Lstm\n",
    "lstmCell = tf.contrib.rnn.LSTMCell(num_units=100, name='Lstm01')\n",
    "_, state = tf.nn.dynamic_rnn(lstmCell, inputVec, scope='DynamicLsmt01', dtype=tf.float32)\n",
    "state=tf.layers.flatten(state[0])\n",
    "predictions = tf.layers.dense(state, 100)\n",
    "print(predictions)\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 100))  \n",
    "\n",
    "loss = tf.losses.mean_squared_error(labels, predictions)            # define loss\n",
    "lossSummary = tf.summary.scalar('Loss', loss)\n",
    "Optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()  \n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_writer = tf.summary.FileWriter('summary\\\\'+time.strftime('%H_%M_%S'), sess.graph)\n",
    "for i in range(10000):\n",
    "    feed_dict={inputVec:[VectorizedQuote[i%101]], labels:[VectorizedAuthor[i%101]]}\n",
    "    _, lossS = sess.run([Optimizer, lossSummary], feed_dict=feed_dict) \n",
    "    train_writer.add_summary(lossS, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100)\n",
      "[array([[ 2.6909828e-03, -1.0077532e-03,  4.0724608e-03, -4.7102203e-03,\n",
      "         1.9833101e-03,  2.0997548e-03,  2.9159258e-03,  1.6461475e-03,\n",
      "         2.3774370e-03, -3.1019381e-04,  2.0521088e-04, -2.3774835e-03,\n",
      "        -2.0391163e-03, -1.5580410e-03, -3.8410937e-03, -1.5315886e-03,\n",
      "         1.6037953e-03, -3.9862311e-03,  5.1049045e-03, -1.6122551e-03,\n",
      "        -1.5756118e-03, -4.1151536e-05,  2.0399985e-03, -2.0520715e-03,\n",
      "        -4.2659817e-03, -2.9546993e-03, -2.4957419e-04, -2.1008663e-03,\n",
      "        -2.4773995e-03,  4.5831269e-03,  2.2566579e-03, -3.7261876e-03,\n",
      "         2.2001520e-03,  7.9800410e-04, -4.1340836e-03,  2.0660758e-03,\n",
      "         5.5802134e-03, -1.0885692e-03,  3.1113864e-03, -6.1354961e-04,\n",
      "        -1.7518646e-03,  1.6543567e-03,  2.2910933e-03,  7.8953052e-04,\n",
      "         3.5451981e-04,  1.4445957e-03,  1.9841820e-03,  1.6877165e-03,\n",
      "        -1.9221640e-03,  4.1528549e-03, -1.8043160e-03,  3.8199387e-03,\n",
      "        -7.9816335e-04,  1.9817194e-03,  8.2618161e-04, -2.1173945e-04,\n",
      "        -3.3644149e-03, -3.2850923e-03, -2.3487713e-03, -2.5766768e-04,\n",
      "        -1.5087285e-03, -2.2390771e-03, -1.2643889e-03,  1.8376762e-03,\n",
      "         2.1044803e-03,  2.8649161e-03,  8.1699528e-04,  9.7351847e-04,\n",
      "         1.7445311e-03, -2.0308199e-03,  4.6304613e-03,  6.7261508e-04,\n",
      "        -3.4374453e-04,  8.5702079e-05, -1.0484061e-04,  2.7180051e-03,\n",
      "        -4.1663526e-03, -1.7806538e-04, -5.4239831e-03, -1.9875371e-03,\n",
      "        -1.0725916e-03,  2.5937015e-03, -4.1413209e-03,  4.6716393e-03,\n",
      "         1.5170395e-03,  9.3413435e-04, -1.7984232e-03, -1.0385748e-03,\n",
      "         1.3932734e-03, -1.5684196e-03,  1.1299655e-03,  2.2554835e-03,\n",
      "        -1.1404824e-03,  3.5307589e-03,  7.5076194e-04, -9.9077355e-04,\n",
      "        -4.7412835e-04, -2.1633634e-03,  8.3594472e-04,  1.1164525e-03]],\n",
      "      dtype=float32)]\n",
      "[('John Wooden', 0.6561616659164429)]\n"
     ]
    }
   ],
   "source": [
    "feed_dict={inputVec:[VectorizedQuote[1]]}\n",
    "print(VectorizedQuote[1].shape)\n",
    "PredVec = sess.run([predictions], feed_dict=feed_dict) \n",
    "print(PredVec)\n",
    "similarWord = model.wv.most_similar(positive=[PredVec[0][0]], topn=1)     # Vector to word\n",
    "print(similarWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked successfully"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
